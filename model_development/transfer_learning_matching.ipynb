{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, precision_score, recall_score, f1_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report, auc, precision_recall_curve, average_precision_score\n",
    "import torch\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check GPU/CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Is CUDA enabled GPU Available?\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Number:\", torch.cuda.device_count())\n",
    "    print(\"Current GPU Index:\", torch.cuda.current_device())\n",
    "    print(\"GPU Type:\", torch.cuda.get_device_name(device=None))\n",
    "    print(\"GPU Capability:\", torch.cuda.get_device_capability(device=None))\n",
    "    print(\"Is GPU Initialized yet?\", torch.cuda.is_initialized())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = \"Landsat\"\n",
    "sat_comb = \"Hypso_filter\"\n",
    "hypso_comb = \"all_filtered_reflectance\"\n",
    "dr_type = \"original\"\n",
    "\n",
    "dir_name = f\"{sensor}/{sat_comb}/{dr_type}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing classification reports if the file exists\n",
    "classification_reports_path = f\"{dir_name}/classification_reports.json\"\n",
    "if os.path.exists(classification_reports_path):\n",
    "    with open(classification_reports_path, \"r\") as json_file:\n",
    "        classification_reports = json.load(json_file)\n",
    "else:\n",
    "    classification_reports = {}\n",
    "\n",
    "model_version = classification_reports[0][0]\n",
    "print(model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(f\"./data/{hypso_comb}_X_train_{sensor}_{sat_comb}.npy\")\n",
    "X_test = np.load(f\"./data/{hypso_comb}_X_test_{sensor}_{sat_comb}.npy\")\n",
    "y_train = np.load(f\"./data/{hypso_comb}_y_train_{dr_type}.npy\")\n",
    "y_test = np.load(f\"./data/{hypso_comb}_y_test_{dr_type}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(data):\n",
    "  L_E = LabelEncoder()\n",
    "  integer_encoded = L_E.fit_transform(data)  \n",
    "  onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "  integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "  one_hot_encoded_data = onehot_encoder.fit_transform(integer_encoded)\n",
    "  return one_hot_encoded_data\n",
    "\n",
    "y_train_encoded = one_hot_encoding(y_train.ravel())\n",
    "y_test_encoded = one_hot_encoding(y_test.ravel())\n",
    "\n",
    "\n",
    "print(y_train_encoded.shape)\n",
    "print(y_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = classification_reports[0][0] \n",
    "model = tf.keras.models.load_model(f\"{dir_name}/{model_version}.keras\")\n",
    "initial_epochs = 50\n",
    "fine_tune_epochs = 20\n",
    "gamma = 4\n",
    "learning_rate = 1e-5\n",
    "trainable_layer_start_idx = -54\n",
    "\n",
    "save_model_version = classification_reports[0][0] + f\"_tl_ie{initial_epochs}_fte{fine_tune_epochs}_g{gamma}_lr{learning_rate}_tbl{trainable_layer_start_idx}\"\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "initial_epochs = 50\n",
    "\n",
    "# Freeze middle layers only\n",
    "for layer in model.layers[:-5]:  # Adjust indices based on your model structure\n",
    "    layer.trainable = False\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=keras.losses.CategoricalFocalCrossentropy(gamma=gamma), \n",
    "                             optimizer=keras.optimizers.RMSprop(), \n",
    "                             metrics=['mse', 'accuracy'])\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, mode='min'), \n",
    "             tf.keras.callbacks.ModelCheckpoint(f\"{dir_name}/{save_model_version}_tl.keras\", verbose=1, monitor='val_loss', save_best_only=True, mode='min')]\n",
    "history = model.fit(X_train, y_train_encoded, epochs=initial_epochs, batch_size=128, verbose=1, validation_split=0.2, shuffle=True, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0,1])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim([0,1])\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Unfreeze middle layers\n",
    "for layer in model.layers[trainable_layer_start_idx:]:  # Adjust indices based on your model structure\n",
    "    if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        layer.trainable = True  # Unfreeze non-BatchNorm layers\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "fine_tune_epochs = 20\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "# Recompile the model with a lower learning rate for fine-tuning\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "              loss=keras.losses.CategoricalFocalCrossentropy(gamma = gamma),\n",
    "                             metrics=['mse', 'accuracy'])\n",
    "\n",
    "# Fine-tune the entire model\n",
    "callbacks_fine = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, mode='min'), \n",
    "             tf.keras.callbacks.ModelCheckpoint(f\"{dir_name}/{save_model_version}_ft.keras\", verbose=1, monitor='val_loss', save_best_only=True, mode='min')]\n",
    "history_fine = model.fit(X_train, y_train_encoded, epochs=total_epochs, initial_epoch=len(history.epoch), batch_size=128, verbose=1, validation_split=0.2, shuffle=True, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions from the Test Set from the Trained Model\n",
    "base_model = tf.keras.models.load_model(f\"{dir_name}/{model_version}.keras\")\n",
    "Predictions_base_model = base_model.predict(X_test, verbose=1)\n",
    "print(Predictions_base_model.shape)\n",
    "\n",
    "# Predictions from the Test Set from the Trained Model\n",
    "best_model_tl = tf.keras.models.load_model(f\"{dir_name}/{save_model_version}_tl.keras\")\n",
    "Predictions = best_model_tl.predict(X_test, verbose=1)\n",
    "print(Predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error of the prediction, one of many evaluation metrics\n",
    "# Using Mean Absolute Error (MAE) in this case as a sample\n",
    "Error = mean_absolute_error(y_test_encoded, Predictions_base_model)\n",
    "print(f\"MAE: {Error}\")\n",
    "Error_tl = mean_absolute_error(y_test_encoded, Predictions)\n",
    "print(f\"MAE_tl: {Error_tl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(history):\n",
    "  # list all dictionaries in history\n",
    "  print(history.history.keys())\n",
    "  # summarize history for error\n",
    "  plt.figure(figsize=(12,10))\n",
    "  plt.subplot(2,1,1)\n",
    "  plt.plot(history.history['mse'])\n",
    "  plt.plot(history.history['val_mse'])\n",
    "  plt.title('Model Error Performance')\n",
    "  plt.ylabel('Error')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper right')\n",
    "  plt.show()\n",
    "  # summarize history for loss\n",
    "  plt.figure(figsize=(12,10))\n",
    "  plt.subplot(2,1,2)\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title('Model Loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper right')\n",
    "  plt.show()\n",
    "#\n",
    "history_plot(history)\n",
    "history_plot(history_fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory if it does not exist\n",
    "\n",
    "if not os.path.exists(dir_name):\n",
    "    os.makedirs(f\"{dir_name}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "y_pred = np.argmax(Predictions, axis=1) + 1\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1-score: {f1}\")\n",
    "\n",
    "report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Load existing classification reports if the file exists\n",
    "classification_reports_path = f\"{dir_name}/classification_reports.json\"\n",
    "if os.path.exists(classification_reports_path):\n",
    "    with open(classification_reports_path, \"r\") as json_file:\n",
    "        classification_reports = json.load(json_file)\n",
    "else:\n",
    "    classification_reports = {}\n",
    "\n",
    "# Add the current model's classification report\n",
    "classification_reports.append([save_model_version, report_dict])\n",
    "\n",
    "# Sort the classification reports by macro F1-score\n",
    "sorted_reports = sorted(\n",
    "    classification_reports,\n",
    "    key=lambda x: x[1][\"macro avg\"][\"f1-score\"],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Save the updated classification reports\n",
    "with open(classification_reports_path, \"w\") as json_file:\n",
    "    json.dump(sorted_reports, json_file, indent=4)\n",
    "\n",
    "class_dict = {\n",
    "    0: \"Spruce\",\n",
    "    1: \"Pine\",\n",
    "    3: \"Deciduous\"\n",
    "}\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_dict.values(), yticklabels=class_dict.values())\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.savefig(f\"{dir_name}/{save_model_version}_confusion_matrix.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
