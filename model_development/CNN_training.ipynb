{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, precision_score, recall_score, f1_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report, auc, precision_recall_curve, average_precision_score\n",
    "import torch\n",
    "import keras\n",
    "import pydot\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check CUDA GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Is CUDA enabled GPU Available?\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Number:\", torch.cuda.device_count())\n",
    "    print(\"Current GPU Index:\", torch.cuda.current_device())\n",
    "    print(\"GPU Type:\", torch.cuda.get_device_name(device=None))\n",
    "    print(\"GPU Capability:\", torch.cuda.get_device_capability(device=None))\n",
    "    print(\"Is GPU Initialized yet?\", torch.cuda.is_initialized())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = \"Hypso\"\n",
    "comb = \"biggerAOI\"\n",
    "dr_type = \"original\"\n",
    "\n",
    "X_train_path = f\"./data/{comb}/X_train.npy\"\n",
    "X_test_path = f\"./data/{comb}/X_test.npy\"\n",
    "y_train_path = f\"./data/{comb}/y_train.npy\"\n",
    "y_test_path = f\"./data/{comb}/y_test.npy\"\n",
    "\n",
    "\n",
    "dir_name = f\"{sensor}/{comb}/{dr_type}_binary\"\n",
    "\n",
    "epochs = 100\n",
    "gamma = 2\n",
    "optimizer_choice = \"rmsprop\"\n",
    "\n",
    "model_version = f\"MobileNet_v3_Small_{epochs}e_focalloss_g{gamma}_{optimizer_choice}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(X_train_path)\n",
    "X_test = np.load(X_test_path)\n",
    "y_train = np.load(y_train_path)\n",
    "y_test = np.load(y_test_path)\n",
    "  \n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "unique_labels, count = np.unique(y_train, return_counts=True)\n",
    "count_classes = len(unique_labels)\n",
    "\n",
    "print(unique_labels)\n",
    "print(count)\n",
    "print(count_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv_1D_block(inputs, model_width, kernel, strides):\n",
    "    # 1D Convolutional Block with BatchNormalization\n",
    "    x = tf.keras.layers.Conv1D(model_width, kernel, strides=strides, padding=\"same\", kernel_initializer=\"he_normal\")(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def Conv_1D_block_2(inputs, model_width, kernel, strides, nl):\n",
    "    # This function defines a 1D convolution operation with BN and activation.\n",
    "    x = tf.keras.layers.Conv1D(model_width, kernel, strides=strides, padding=\"same\", kernel_initializer=\"he_normal\")(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    if nl == 'HS':\n",
    "        x = x * tf.keras.activations.relu(x + 3.0, max_value=6.0) / 6.0\n",
    "    elif nl == 'RE':\n",
    "        x = tf.keras.activations.relu(x, max_value=6.0)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def Conv_1D_DW(inputs, model_width, kernel, strides, alpha):\n",
    "    # 1D Depthwise Separable Convolutional Block with BatchNormalization\n",
    "    model_width = int(model_width * alpha)\n",
    "    x = tf.keras.layers.SeparableConv1D(model_width, kernel, strides=strides, depth_multiplier=1, padding='same')(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Conv1D(model_width, 1, strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def bottleneck_block(inputs, filters, kernel, t, alpha, s, r=False):\n",
    "    tchannel = tf.keras.backend.int_shape(inputs)[-1] * t\n",
    "    cchannel = int(filters * alpha)\n",
    "\n",
    "    x = Conv_1D_block(inputs, tchannel, 1, 1)\n",
    "    x = tf.keras.layers.SeparableConv1D(filters, kernel, strides=s, depth_multiplier=1, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Conv1D(cchannel, 1, strides=1, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('linear')(x)\n",
    "\n",
    "    if r:\n",
    "        x = tf.keras.layers.concatenate([x, inputs], axis=-1)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def bottleneck_block_2(inputs, filters, kernel, e, s, squeeze, nl, alpha):\n",
    "    # This function defines a basic bottleneck structure.\n",
    "\n",
    "    input_shape = tf.keras.backend.int_shape(inputs)\n",
    "\n",
    "    tchannel = int(e)\n",
    "    cchannel = int(alpha * filters)\n",
    "\n",
    "    r = s == 1 and input_shape[2] == filters\n",
    "\n",
    "    x = Conv_1D_block_2(inputs, tchannel, 1, 1, nl)\n",
    "\n",
    "    x = tf.keras.layers.SeparableConv1D(filters, kernel, strides=s, depth_multiplier=1, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    if nl == 'HS':\n",
    "        x = x * tf.keras.activations.relu(x + 3.0, max_value=6.0) / 6.0\n",
    "    if nl == 'RE':\n",
    "        x = tf.keras.activations.relu(x, max_value=6.0)\n",
    "\n",
    "    if squeeze:\n",
    "        x = _squeeze(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(cchannel, 1, strides=1, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    if r:\n",
    "        x = tf.keras.layers.Add()([x, inputs])\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def inverted_residual_block(inputs, filters, kernel, t, alpha, strides, n):\n",
    "    if strides == 1:\n",
    "        x = bottleneck_block(inputs, filters, kernel, t, alpha, strides, True)\n",
    "    else:\n",
    "        x = bottleneck_block(inputs, filters, kernel, t, alpha, strides)\n",
    "\n",
    "    for i in range(1, n):\n",
    "        x = bottleneck_block(x, filters, kernel, t, alpha, 1, True)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def _squeeze(inputs):\n",
    "    # This function defines a squeeze structure.\n",
    "\n",
    "    input_channels = int(inputs.shape[-1])\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(inputs)\n",
    "    x = tf.keras.layers.Dense(input_channels, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(input_channels, activation='hard_sigmoid')(x)\n",
    "    x = tf.keras.layers.Reshape((1, input_channels))(x)\n",
    "    x = tf.keras.layers.Multiply()([inputs, x])\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class MobileNet:\n",
    "    def __init__(self, length, num_channel, num_filters, problem_type='Regression',\n",
    "                 output_nums=1, pooling='avg', dropout_rate=False, alpha=1.0):\n",
    "        self.length = length\n",
    "        self.num_channel = num_channel\n",
    "        self.num_filters = num_filters\n",
    "        self.problem_type = problem_type\n",
    "        self.output_nums = output_nums\n",
    "        self.pooling = pooling\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def MLP(self, x):\n",
    "        if self.pooling == 'avg':\n",
    "            x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "        elif self.pooling == 'max':\n",
    "            x = tf.keras.layers.GlobalMaxPool1D()(x)\n",
    "        # Final Dense Outputting Layer for the outputs\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        if self.dropout_rate:\n",
    "            x = tf.keras.layers.Dropout(self.dropout_rate)(x)\n",
    "        outputs = tf.keras.layers.Dense(self.output_nums, activation='linear')(x)\n",
    "        if self.problem_type == 'Classification':\n",
    "            outputs = tf.keras.layers.Dense(self.output_nums, activation='softmax')(x)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def MobileNet_v1(self):\n",
    "        inputs = tf.keras.Input((self.length, self.num_channel))\n",
    "\n",
    "        x = Conv_1D_block(inputs, self.num_filters * (2 ** 0), 3, 2)\n",
    "        x = Conv_1D_DW(x, self.num_filters, 3, 1, self.alpha)\n",
    "        x = Conv_1D_DW(x, self.num_filters * (2 ** 1), 3, 2, self.alpha)\n",
    "        x = Conv_1D_DW(x, self.num_filters, 3, 1, self.alpha)\n",
    "        x = Conv_1D_DW(x, self.num_filters * (2 ** 2), 3, 2, self.alpha)\n",
    "        x = Conv_1D_DW(x, self.num_filters, 3, 1, self.alpha)\n",
    "        x = Conv_1D_DW(x, self.num_filters * (2 ** 3), 3, 2, self.alpha)\n",
    "        for i in range(5):\n",
    "            x = Conv_1D_DW(x, self.num_filters, 3, 1, self.alpha)\n",
    "        x = Conv_1D_DW(x, self.num_filters * (2 ** 4), 3, 2, self.alpha)\n",
    "        x = Conv_1D_DW(x, self.num_filters * (2 ** 5), 3, 2, self.alpha)\n",
    "\n",
    "        outputs = self.MLP(x)\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def MobileNet_v2(self):\n",
    "        inputs = tf.keras.Input((self.length, self.num_channel))\n",
    "        x = Conv_1D_block(inputs, self.num_filters, 3, 2)\n",
    "\n",
    "        x = inverted_residual_block(x, 16, 3, t=1, alpha=self.alpha, strides=1, n=1)\n",
    "        x = inverted_residual_block(x, 24, 3, t=6, alpha=self.alpha, strides=2, n=2)\n",
    "        x = inverted_residual_block(x, 32, 3, t=6, alpha=self.alpha, strides=2, n=3)\n",
    "        x = inverted_residual_block(x, 64, 3, t=6, alpha=self.alpha, strides=2, n=4)\n",
    "        x = inverted_residual_block(x, 96, 3, t=6, alpha=self.alpha, strides=1, n=3)\n",
    "        x = inverted_residual_block(x, 160, 3, t=6, alpha=self.alpha, strides=2, n=3)\n",
    "        x = inverted_residual_block(x, 320, 3, t=6, alpha=self.alpha, strides=1, n=1)\n",
    "        x = Conv_1D_block(x, 1280, 1, 1)\n",
    "\n",
    "        outputs = self.MLP(x)\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def MobileNet_v3_Small(self):\n",
    "        inputs = tf.keras.Input((self.length, self.num_channel))\n",
    "\n",
    "        x = Conv_1D_block_2(inputs, 16, 3, strides=2, nl='HS')\n",
    "        x = bottleneck_block_2(x, 16, 3, e=16, s=2, squeeze=True, nl='RE', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 24, 3, e=72, s=2, squeeze=False, nl='RE', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 24, 3, e=88, s=1, squeeze=False, nl='RE', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 40, 5, e=96, s=2, squeeze=True, nl='HS', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 40, 5, e=240, s=1, squeeze=True, nl='HS', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 40, 5, e=240, s=1, squeeze=True, nl='HS', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 48, 5, e=120, s=1, squeeze=True, nl='HS', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 48, 5, e=144, s=1, squeeze=True, nl='HS', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 96, 5, e=288, s=2, squeeze=True, nl='HS', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 96, 5, e=576, s=1, squeeze=True, nl='HS', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 96, 5, e=576, s=1, squeeze=True, nl='HS', alpha=self.alpha)\n",
    "        x = Conv_1D_block_2(x, 576, 1, strides=1, nl='HS')\n",
    "        x = x * tf.keras.activations.relu(x + 3.0, max_value=6.0) / 6.0\n",
    "        x = tf.keras.layers.Conv1D(1280, 1, padding='same')(x)\n",
    "        \n",
    "        outputs = self.MLP(x)\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def MobileNet_v3_Large(self):\n",
    "        inputs = tf.keras.Input((self.length, self.num_channel))\n",
    "\n",
    "        x = Conv_1D_block_2(inputs, 16, 3, strides=2, nl='HS')\n",
    "        x = bottleneck_block_2(x, 16, 3, e=16, s=1, squeeze=False, nl='RE', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 24, 3, e=64, s=2, squeeze=False, nl='RE', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 24, 3, e=72, s=1, squeeze=False, nl='RE', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 40, 5, e=72, s=2, squeeze=True, nl='RE', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 40, 5, e=120, s=1, squeeze=True, nl='RE', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 40, 5, e=120, s=1, squeeze=True, nl='RE', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 80, 5, e=240, s=2, squeeze=False, nl='HS', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 80, 3, e=200, s=1, squeeze=False, nl='HS', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 80, 3, e=184, s=1, squeeze=False, nl='HS', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 80, 3, e=184, s=1, squeeze=False, nl='HS', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 112, 3, e=480, s=1, squeeze=True, nl='HS', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 112, 3, e=672, s=1, squeeze=True, nl='HS', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 160, 5, e=672, s=2, squeeze=True, nl='HS', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 160, 5, e=960, s=1, squeeze=True, nl='HS', alpha=self.alpha)\n",
    "        x = bottleneck_block_2(x, 160, 5, e=960, s=1, squeeze=True, nl='HS', alpha=self.alpha)\n",
    "        x = Conv_1D_block_2(x, 960, 1, strides=1, nl='HS')\n",
    "        x = x * tf.keras.activations.relu(x + 3.0, max_value=6.0) / 6.0\n",
    "        x = tf.keras.layers.Conv1D(1280, 1, padding='same')(x)\n",
    "\n",
    "        outputs = self.MLP(x)\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(data):\n",
    "  L_E = LabelEncoder()\n",
    "  integer_encoded = L_E.fit_transform(data)  \n",
    "  onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "  integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "  one_hot_encoded_data = onehot_encoder.fit_transform(integer_encoded)\n",
    "  return one_hot_encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded = one_hot_encoding(y_train.ravel())\n",
    "y_test_encoded = one_hot_encoding(y_test.ravel())\n",
    "\n",
    "\n",
    "print(y_train_encoded.shape)\n",
    "print(y_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Configurations for the 1D Network in Classification Mode\"\n",
    "length = X_train.shape[1]       # Number of Features (or length of the signal)\n",
    "model_width = 64                # Number of Filter or Kernels in the Input Layer (Power of 2 to avoid error)\n",
    "num_channel = 1                 # Number of Input Channels\n",
    "problem_type = 'Classification' # Regression or Classification\n",
    "class_number = count_classes # Number of Output Class in Classification Mode (>=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classification_Model = MobileNet(length, num_channel, model_width, problem_type=problem_type, output_nums=class_number).MobileNet_v3_Small()\n",
    "optimizer = keras.optimizers.Adam() if optimizer_choice == 'adam' else keras.optimizers.RMSprop()\n",
    "Classification_Model.compile(loss=keras.losses.CategoricalFocalCrossentropy(gamma=gamma), \n",
    "                             optimizer=optimizer, \n",
    "                             metrics=['mse', 'accuracy'])\n",
    "# loss=keras.losses.CategoricalFocalCrossentropy(gamma=gamma)\n",
    "# keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification_Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping and Model_Checkpoints are optional parameters\n",
    "# Early Stopping is to stop the training based on certain condition set by the user\n",
    "# Model Checkpoint is to save a model in a directory based on certain conditions so that it can be used later for Transfer Learning or avoiding retraining\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(f\"{dir_name}/{model_version}.keras\", verbose=1, monitor='val_loss', save_best_only=True, mode='min')]\n",
    "history = Classification_Model.fit(X_train, y_train_encoded, epochs=epochs, batch_size=128, verbose=1, validation_split=0.2, shuffle=True, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions from the Test Set from the Trained Model\n",
    "best_model = tf.keras.models.load_model(f\"{dir_name}/{model_version}.keras\")\n",
    "# model_version = \"gamma4_lr0.0001_rmsprop\"\n",
    "#best_model = tf.keras.models.load_model(f\"{tuner_dir}/gridsearch_model_trial_13.keras\")\n",
    "#tf.keras.models.save_model(best_model, f\"{dir_name}/{model_version}.keras\")\n",
    "\n",
    "Predictions = Classification_Model.predict(X_test, verbose=1)\n",
    "#Predictions = best_model.predict(X_test, verbose=1)\n",
    "\n",
    "print(Predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error of the prediction, one of many evaluation metrics\n",
    "# Using Mean Absolute Error (MAE) in this case as a sample\n",
    "Error = mean_absolute_error(y_test_encoded, Predictions)\n",
    "print(f\"MAE: {Error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(history):\n",
    "  # list all dictionaries in history\n",
    "  print(history.history.keys())\n",
    "  # summarize history for error\n",
    "  plt.figure(figsize=(12,10))\n",
    "  plt.subplot(2,1,1)\n",
    "  plt.plot(history.history['mse'])\n",
    "  plt.plot(history.history['val_mse'])\n",
    "  plt.title('Model Error Performance')\n",
    "  plt.ylabel('Error')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper right')\n",
    "  plt.show()\n",
    "  # summarize history for loss\n",
    "  plt.figure(figsize=(12,10))\n",
    "  plt.subplot(2,1,2)\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title('Model Loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper right')\n",
    "  plt.show()\n",
    "#\n",
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory if it does not exist\n",
    "\n",
    "if not os.path.exists(dir_name):\n",
    "    os.makedirs(f\"{dir_name}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# y_pred = np.argmax(Predictions, axis=1) + 1\n",
    "y_pred = np.argmax(Predictions, axis=1)\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1-score: {f1}\")\n",
    "\n",
    "report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Load existing classification reports if the file exists\n",
    "classification_reports_path = f\"{dir_name}/classification_reports.json\"\n",
    "if os.path.exists(classification_reports_path):\n",
    "    with open(classification_reports_path, \"r\") as json_file:\n",
    "        classification_reports = json.load(json_file)\n",
    "else:\n",
    "    classification_reports = []\n",
    "\n",
    "# Add the current model's classification report\n",
    "classification_reports.append([model_version, report_dict])\n",
    "\n",
    "# Sort the classification reports by macro F1-score\n",
    "sorted_reports = sorted(\n",
    "    classification_reports,\n",
    "    key=lambda x: x[1][\"macro avg\"][\"f1-score\"],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Save the updated classification reports\n",
    "with open(classification_reports_path, \"w\") as json_file:\n",
    "    json.dump(sorted_reports, json_file, indent=4)\n",
    "\n",
    "\"\"\" class_dict = {\n",
    "    0: \"Spruce\",\n",
    "    1: \"Pine\",\n",
    "    3: \"Deciduous\"\n",
    "}\"\"\"\n",
    "\n",
    "class_dict = {\n",
    "    0: \"Conifer\",\n",
    "    1: \"Deciduous\"\n",
    "}\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_dict.values(), yticklabels=class_dict.values())\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.savefig(f\"{dir_name}/{model_version}_confusion_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the scoreboard\n",
    "print(\"Scoreboard (sorted by Macro F1-Score):\")\n",
    "print(\"{:<20} {:<10}\".format(\"Model Version\", \"Macro F1-Score\"))\n",
    "print(\"-\" * 30)\n",
    "for model_version, report in sorted_reports:\n",
    "    print(\"{:<20} {:.4f}\".format(model_version, report[\"macro avg\"][\"f1-score\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "import os\n",
    "import shutil  # For resetting KerasTuner directory\n",
    "\n",
    "# Clear previous KerasTuner trials (fixes corrupted search spaces)\n",
    "tuner_dir = f\"{dir_name}/kerastuner_results\"\n",
    "i = 1\n",
    "while os.path.exists(tuner_dir):\n",
    "    tuner_dir = f\"{dir_name}/kerastuner_results_{i}\"\n",
    "    i += 1\n",
    "\n",
    "# Function to build the model\n",
    "def build_model(hp):\n",
    "    model = MobileNet(length, num_channel, model_width, problem_type=problem_type, output_nums=class_number).MobileNet_v3_Small()\n",
    "    \n",
    "    # Ensure gamma is valid for focal loss\n",
    "    gamma = hp.Choice('gamma', [0, 2, 4])\n",
    "    learning_rate = hp.Choice('learning_rate', [1e-4, 1e-3, 1e-2])\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate) if optimizer_choice == 'adam' else keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    \n",
    "    # Compile with correct gamma\n",
    "    model.compile(\n",
    "        loss=keras.losses.CategoricalFocalCrossentropy(gamma=gamma), \n",
    "        optimizer=optimizer, \n",
    "        metrics=['mse', 'accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Define tuner (change to RandomSearch if needed)\n",
    "tuner = kt.GridSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,  # Number of combinations to try\n",
    "    executions_per_trial=1,  # Can increase to average results\n",
    "    directory=tuner_dir,  # Use a clean directory\n",
    "    project_name=\"gridsearch_tuning\"\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "tuner.search(\n",
    "    X_train, y_train_encoded,\n",
    "    epochs=20,  \n",
    "    validation_split=0.2,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min')]\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best Hyperparameters: {best_hps.values}\")\n",
    "\n",
    "# Retrieve the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Save the best model\n",
    "best_model.save(f\"{tuner_dir}/best_gridsearch_model.keras\")\n",
    "\n",
    "# Save all models from the search\n",
    "for i, trial in enumerate(tuner.oracle.trials.values()):\n",
    "    model_path = f\"{tuner_dir}/gridsearch_model_trial_{i}.keras\"\n",
    "    trial_model = tuner.get_best_models(num_models=1)[0]  # Get the trial model\n",
    "    trial_model.save(model_path)\n",
    "    print(f\"Saved: {model_path}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Print all trial results for analysis\n",
    "print(\"\\n===== Hyperparameter Trials and Validation Loss =====\\n\")\n",
    "\n",
    "# Sort trials by validation loss (best to worst)\n",
    "sorted_trials = sorted(tuner.oracle.trials.values(), key=lambda x: x.metrics.get_best_value('val_loss') if x.metrics.get_best_value('val_loss') is not None else float('inf'))\n",
    "\n",
    "for i, trial in enumerate(sorted_trials):\n",
    "    trial_id = trial.trial_id\n",
    "    trial_hparams = trial.hyperparameters.values\n",
    "    val_loss = trial.metrics.get_best_value('val_loss')  # Get best val_loss for this trial\n",
    "    val_mse = trial.metrics.get_best_value('val_mse')\n",
    "    val_accuracy = trial.metrics.get_best_value('val_accuracy')\n",
    "\n",
    "    print(f\"Rank {i+1}:\")\n",
    "    print(f\"  Trial ID: {trial_id}\")\n",
    "    print(f\"  Hyperparameters: {trial_hparams}\")\n",
    "    print(f\"  Best Validation Loss: {val_loss:.6f}\\n\")\n",
    "    print(f\"  Best Validation MSE: {val_mse:.6f}\\n\")\n",
    "    print(f\"  Best Validation Accuracy: {val_accuracy:.6f}\\n\")\n",
    "\n",
    "\n",
    "print(\"=====================================================\")\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
